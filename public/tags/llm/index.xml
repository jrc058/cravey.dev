<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llm on Cravey.dev</title><link>https://cravey.dev/tags/llm/</link><description>Recent content in Llm on Cravey.dev</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://cravey.dev/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://cravey.dev/docs/projects/ollama/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://cravey.dev/docs/projects/ollama/</guid><description>&lt;h1 id="local-ai-with-ollama-and-open-webui"&gt;Local AI with Ollama and Open WebUI&lt;a class="anchor" href="#local-ai-with-ollama-and-open-webui"&gt;#&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Running AI models locally instead of sending every query to OpenAI or Anthropic was the goal. &lt;a href="https://ollama.ai/"&gt;Ollama&lt;/a&gt; makes it possible to run large language models on your own hardware, and &lt;a href="https://openwebui.com/"&gt;Open WebUI&lt;/a&gt; provides a ChatGPT-like interface for interacting with them.&lt;/p&gt;
&lt;p&gt;This setup means I can experiment with AI without worrying about API costs, rate limits, or sending sensitive data to third parties. The models run entirely on my server, using the GPU for acceleration.&lt;/p&gt;</description></item></channel></rss>