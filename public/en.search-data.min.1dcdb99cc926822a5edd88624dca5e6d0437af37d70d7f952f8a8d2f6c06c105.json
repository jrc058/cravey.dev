[{"id":0,"href":"/docs/about/","title":"About","section":"Docs","content":"John# Professionally I\u0026rsquo;ve worn many hats from my current role at Amazon in Customer Success, my previos role as a Weapons of Mass Destruction (WMD) Advisor in the military, and if you go back far enough I did door-to-door sales, snowbummed in Vail, volunteered for an earth home project, and a lot in between.\nUnprofessionally, I still wear many hats. Almost all towards digital sovereignty and privacy advocacy, and anything for self-sustaining and building a worldly skillset that can apply to most any project I take on.\nLinks# linkedin \u0026hellip; and that\u0026rsquo;s pretty much it. email john@cravey.dev\n"},{"id":1,"href":"/docs/projects/","title":"Projects","section":"Docs","content":"Projects# This portfolio is a mix of professional and enthusiast work.\nI enjoy a lot of different work, at different levels of skill. My background in website building is fairly amateur, and most any other code is vibe-coded with the aid of Amazon Kiro, Google Gemini, and my local LLMs, usually qwen3:14b.\nThe aid of AI has had immense impact on my continuing education. The military abstained from AI when I was in it, and at the time I got out of the military the landscape of AI had completely changed. Where a lot of peers felt boy-who-cried-wolf about AI and dismissed later generations, I got the chance to dive in during a major leap in the LLM space.\nSo, I incorporated it into everything, quickly. I started Obsidian notes and pretty quickly let Kiro run loose in that vault. That led to a lot of file creation and a lot of slop, admittedly. It probably didn\u0026rsquo;t even save time, at first, maybe. However, it was a project multiplier.\nAll of my projects I have always had backburnered immediately bubbled up and saw light. The computer I replaced became a server, and the new computer was my workstation for all of two months before I turned my new computer into the server because I needed more. Just more.\nNow, my homelab hosts:\nHome Assistant My Project Zigbee2MQTT ZwaveJS Immich OpenWebui (potentially changing to llms.py) Navidrome Ollama Searxng instance WebDav Jellyfin Miniflux Code-server Minecraft server Fun project to get my nephew\u0026rsquo;s Bedrock game to my Java game with geyser and floodgate Homarr Syncthing Wyoming Openwakeword, piper, whisper Calibre Web Rotki Several websites \u0026hellip; and the list goes to a revolving doors of projects I\u0026rsquo;ve tried, like; fava/beancount ghostfolio and more Self-hosting a server has led me to a world of understanding that no one could have ever taught me. And AI does the heavy lifting, because time is hard to come by. Even when I started using Linux 15 years ago, I still always just googled commands to copy and paste into the command line\u0026hellip; so nothing\u0026rsquo;s changed.\nThere\u0026rsquo;s been hours of correcting slop, and hours of having to go into the developer\u0026rsquo;s docs. It\u0026rsquo;s not a set-it-and-forget-it workflow, using AI at this stage.\nBut step-by-step, walking hand-in-hand with AI, I correct it, it corrects my understanding, there\u0026rsquo;s corrections on both sides. And that correction from the other side, pooling for trained data of documentation and other user use cases, has multiplied my project productivity, even outside of just the server.\nThe wife and I made a deep water culture garden, which was really set up from a few DIY instruction sites, but a lot of the maintenance is led by AI.\n"},{"id":2,"href":"/docs/posts/","title":"Posts","section":"Docs","content":"Not sure how to create a static page here to dynamically display current posts.\n"},{"id":3,"href":"/docs/projects/cyberdecktech/","title":"CyberDeck Tech","section":"Projects","content":"CyberDeck Tech# CyberDeck Tech is a project dedicated to exploring the philosophy of cyberdecks and digital sovereignty. It serves as a curated digital garden for ideas around privacy-first mobile computing and local AI inference.\nTechnologies Used# Jekyll ReactJS Vibe coded with Amazon Kiro Site Creation# The central concept of the site are obsidian notes I made about the Steam Deck and cyberdecks as a whole- the ecosystem of a touch screen device that is not vendor locked to a mobile operating system, with a novel form factor that has potential for more utility.\nI handed my notes over to Amazon Kiro and it generated several files to design the architecture of the website, and then let it loose with react and jekyll to build the site. The humble beginnings visually look very similar to how it remains. The largest amount of edits and rebuilds were for the Map of Content navigation for the \u0026ldquo;connected concepts\u0026rdquo; navigation, the hover elements, and the tile cards to look like terminal UIs.\nIt probably took 20 solid iterations of rebuilding the site, of reworking major portions of it, then a lot of random tweaking for small edits, but over the course of a week it was done without me actually writing any of the code. The most technical extent I went to was pulling up the developer console on a browser to copy and paste error codes into Kiro for Kiro to diagnose the problems and create a solution.\nSite Result# Aside from some JS bugs, the site seems to load pretty well. It\u0026rsquo;s chunked in blocks that dynamically load so the whole website doesn\u0026rsquo;t have to load at once and it can be served faster. Should probably run a benchmark to see if it\u0026rsquo;s fast, but it\u0026rsquo;s fast and responsive so I haven\u0026rsquo;t really touched it since.\nIt was made to be an ongoing post and publication, but it\u0026rsquo;s a pretty niche project that I work on and I\u0026rsquo;m happy having it as a more static content site for the project than anything. It\u0026rsquo;s less so the project, and really more the core concept of why advocating for cyberdecks is pro sovereignty and releases us from vendor lock.\nFuture State# I don\u0026rsquo;t know. I like the concept of cyberdecks. I continue to make changes to my Steam Deck in line with the philosophy, I just don\u0026rsquo;t update the site. The workflow is kind of convoluted, and I should migrate it to my current workflow on code-server and goose cli, once I migrate it into the same workflow I use for cravey.dev I will likely compartmentalize publications better and be able to update it more easily.\n"},{"id":4,"href":"/docs/projects/homeassistant/","title":"Homeassistant","section":"Projects","content":"Home Assistant Smart Home# Home Assistant is a great tool for smart automations, run locally. I learned more about my data and my home from using Home Assistant more than most things. It can be really plug and play or super customizable depending on how you are, which might be a turn off for most, wife approval is a leading cause of failed Home Assistant projects.\nThe project started with help from Kiro, and somewhere along the line became a fun project to use local LLMs for, as well as learning some JSON myself, though I am partial to GUI.\nHow We Built It# Kiro and I started with a docker container on the server. Leaving it at just that would be simple enough, downloading the integrations and just using it like that. I wanted a Zigbee network and a Z-Wave network so I could be pretty agnostic. There are mixed reviews about Thread/Matter and Home Assistant, so I will wait for more support before migrating. As it stands, I\u0026rsquo;m really pleased with zigbee and zwave.\nKiro managed to make a lot of automations, with some back and forth for syntax. I gave Kiro access to all the entities and also let it come up with common, low hanging fruit to better my quality of life.\nSome Core Entities and Integrations# It\u0026rsquo;s pretty innumerable, I think there\u0026rsquo;s over 800 entities in my house. Which I guess is numerating it, but the acceleration that number grows feels unreachable.\nMost Common# Lighting# Philips Hue bulbs for lights. I wanted to be anti-Philips but these expose a lot of entities which means more features. The ease of integration was simple, zigbee interviews. I have 8 bulbs: - 2 in the garage turn on for an hour when my garage contact sensor detects the door opens - 2 nightstands in the primary - 4 lamps around the house I\u0026rsquo;m big on small lights, not the big lights. So, naturally I can\u0026rsquo;t say Philips without mentioning:\nAdaptive Lighting is absolutely necessary. Not because it needs it, but because I need it. It syncs with the sun.sun entity and uses some fancy parabolic timing sequence to calculate perfect light exposure for time of day. Throughout the day, the lights get colder or warmer depending on the time, with a lot of customizing presets if you like.\nSecurity# Ring Ecosystem\u0026hellip; Love hate relationship. I live a life of contradictions, and I work at Amazon but don\u0026rsquo;t want Ring. But my house came with it so I use it. The Ring integration is lacking because Ring relies on it\u0026rsquo;s own proprietary platform- which don\u0026rsquo;t get me wrong is actually really nice and adds a lot of features actively, but I want all my services on one dashboard.\nI\u0026rsquo;m very tired of being fragmented around services and apps and dashboards, I just want everything on my own service. Ring accomplishes that. The camera is it\u0026rsquo;s own integration, with limited capabilities. I\u0026rsquo;ve tried RSTP with a Ring MQTT, but the service really just doesn\u0026rsquo;t stream data. I have a Ring Pro account complementary because of the home plan, but the service does not stream the feed. I\u0026rsquo;ve had several minor successes, but it\u0026rsquo;s too broken to work. It does take snapshots, and has a few exposed entities, just no live stream.\nRing alarm/hub, contact sensors, and keypad were found on the MQTT integration and are relatively fully featured. Can set modes, schedules, chirps, tones, etc. The contact sensors are pretty handy devices and I\u0026rsquo;d recommend.\nCompanion Apps# The companion apps allow for exposing phone entities and are assigned as the \u0026ldquo;Person\u0026rdquo;. Android and iPhone experiences vary. Android exposed about 200 entities, iPhone something like 20. Android, or at least my GrapheneOS, do not host geolocation and I had to get a separate app to serve geolocation data to the server. iPhone works natively. So, experiences varies to say the least.\nBut it allows for notifications to be pushed, on completion of an automation or when an entity changes. When the \u0026ldquo;Person\u0026rdquo; gets in a geofence, automations can start, pretty configurable especially with security, lighting, energy, etc.\nClimate# The ZWave Honeywell Home came with the house. It was finicky at first, but I think it\u0026rsquo;s finally settled. It took some tinkering with wiring direct or battery powered, I think it likes battery powered if I remember, which doesn\u0026rsquo;t let it be a repeater for the zwave network unfortunately.\nOddity Entities# Torque Pro# OBDII adapter and Torque Pro is a crazy combination for Home Assistant. This was all Kiro. The OBDII sensor data exposed over 200 entities, but they aren\u0026rsquo;t displayed in the entities list because it\u0026rsquo;s a custom code integration.\nIt shows everything from G-forces trip mileage to service information. It\u0026rsquo;s a new-to-me integration and I have yet to digest the data, I just have a dashboard with a bunch of graphs that show different entities/sensors.\nThere\u0026rsquo;s frustrations with the integration because the Torque phone app says the units are in miles but everything is still in kilometers. So, I\u0026rsquo;m working through what the issue is, and if I can\u0026rsquo;t find the issue I\u0026rsquo;m going to build a helper that converts the data to miles.\nBut once I get it up and running, the plan is to build out service reminders because I just dismiss that notification in the vehicle. Outside of service reminders, there\u0026rsquo;s diagnostic information in the app itself, but I\u0026rsquo;d also like to have Home Assistant automatically make a maintenance log for my own records. Home Assistant will keep a record of the OBDII data points, which is nice because the OBDII adapter itself is just measuring live data. So the logs can live on Home Assistant, and with the service codes from the OBDII adapter, I can feed all that data to an LLM to help troubleshoot in the future. This will likely be a triage for vehicle problems before going to a mechanic.\nSmartHub Energy# My utility provider does not integrate with Home Assistant, but SmartHub Energy creatively gets around API usage and requires some browser console digging to get it up and working.\nIt only exposes usage rates, but integrates with the energy dashboard just fine and gives me insight into something I never look at because it\u0026rsquo;s on a separate proprietary dashboard on another webpage that needs another login, about 5 steps too many for me to check frequently. This is just on my dashboard now.\nOthers# I have a long list, but some other major integrations (native or through HACS):\nAndroid TV Remote CalDAV Ecowitt \u0026lt;\u0026ndash; Love, the company seems very Home Assistant friendly ESPHome Home Maintenance Immich - would like to use immich facial recognition with cameras Jellyfin - haven\u0026rsquo;t really figured out much for this since I host jellyfin on another domain Local Calendar MQTT OwnTracks - Android geolocation Rain Bird - sprinklers Tailscale \u0026hellip; and a lot of others. What I Learned# Kiro did a lot of the heavy lifting, especially in the beginning. Part of that heavy lifting was just teaching me- not the platform itself, because that can all be done in the GUI, but with the config files and making it feel much more me. Custom dashboard creation got to be a breeze, just giving the entities I want, how I want it, and copying and pasting. Boom. Brand new dashboard.\nI got decidedly involved in the house. I requested an electrical diagram and blueprint from the architect of the house, which was a hassle to get thanks to Lennar being the way they are. I got the diagram for something else a while back, but it got be thinking about electrical and smart devices. I learned a lot about retrofitting existing dumb devices, like normal light switches, with smart devices like shelly switches that live behind the switch, which I plan to get for the big lights.\nI learned the most about networking technologies. MQTT, RSTP, WebRTC. Dockerized set up and containers speaking to containers. Getting ollama to talk was especially fun\u0026hellip;\nThe Voice Assistant integration is still a work in progress, but it\u0026rsquo;s forced me to learn about STT and TTS models and how they work.\nCurrent State# AI has been pretty hands-off since Home Assistant has become more established on the server. I have a cronjob that takes all the entities daily and adds it to my server documentation folder, and that folder is in my goose cli workspace so my AI always has latest entities.\nI have dozens of automations routinely going. Lights turn on ambiently before my alarm goes off, turn off automatically when no one is home, turn back on when people are home, and turn off when it\u0026rsquo;s bedtime. Security automations maintain alarm automatically so it arms away or home depending on a factor of environments.\nDashboards show a great view at a glance, I haven\u0026rsquo;t touched my main dashboard in ages now. It has lights, alarm, and critical things at a glance in a wife approved interface.\nThe home really is set up more than I had anticipated, and I could leave it as is and be perfectly content. However, my nature prevents that. It will continue being an ongoing project.\nWhat\u0026rsquo;s Next# Voice control needs hardware deployment—the software is ready but need more Atom Echos placed around the house. I want to expand to more motion detection. There\u0026rsquo;s more recent technology with Wi-Fi detecting presence similar to mmWave technology.\n"},{"id":5,"href":"/docs/projects/immich/","title":"Immich","section":"Projects","content":"Immich Photo Management# Google Photos is convenient until you realize they have every photo you\u0026rsquo;ve ever taken. I wanted that convenience without giving up my privacy, so I deployed Immich—a self-hosted photo management system that looks and feels like Google Photos but runs local.\nThe interface is nearly identical to commercial photo services. Upload photos, automatic organization, facial recognition, albums, sharing—all the features you\u0026rsquo;d expect. The difference is that everything stays on my hardware, and I control who has access.\nHow It Works# Immich runs as a set of Docker containers on my home server. There\u0026rsquo;s the main server, a PostgreSQL database for metadata, a machine learning container for facial recognition and object detection, and a Redis cache for performance. It\u0026rsquo;s more complex than some of my other services, but the architecture makes sense once you understand what each piece does.\nThe mobile app connects to my server and automatically backs up photos in the background. I can browse my entire library from my phone, create albums, and share links to specific collections. When I take photos at a family event, I create a shared album and send a link—anyone with that link can view and download the original images with no compression. Anyone in my family knows how much I hate compression over texting. Pictures should never be sent over text.\nStorage lives on my server\u0026rsquo;s data drive, with the actual photo files separate from the database. This makes backups straightforward and means I can move the photos to different storage if needed without rebuilding the database.\nWorking with Kiro# Setting up Immich was more involved than simpler services like Navidrome. The multi-container architecture meant coordinating several services and making sure they could all talk to each other. Kiro helped me understand the Docker Compose configuration and how the containers network together.\nWe hit a snag with the admin interface freezing the browser. Turns out it was a bug in the version I was running—the backend was fine, but the frontend JavaScript had an issue. Kiro helped me troubleshoot by checking logs and testing different scenarios until we figured out it was a known bug fixed in a newer version. Updating solved it.\nPassword resets were another learning experience. When I locked myself out of an account (guilty), we had to go directly into the PostgreSQL database to reset the password. Kiro walked me through generating a bcrypt hash and updating the database safely, which taught me more about how authentication works under the hood.\nWhat I Learned# Facial recognition on your own hardware is surprisingly good. Immich\u0026rsquo;s machine learning container can identify people in photos and group them automatically. It\u0026rsquo;s not perfect, but it\u0026rsquo;s good enough to be useful, and it all happens locally without sending photos to some cloud service.\nThe mobile app is solid. Background uploads work reliably, and the interface is responsive. It feels like using a commercial app, which is rare for self-hosted software.\nSharing is well-designed. You can create a link to an album that expires after a certain time or number of views. People can view and download photos without needing an account. It\u0026rsquo;s exactly what I need for sharing event photos with family. I have not set up users outside of me and my wife, but I have been trying to persuade my family to migrate to my server for all their photos, too. If I win that battle, I will create users and manage permissions a bit better than I am now, but I would be happy to take all their data off of Apple and Google servers. I need more comfortable redundancy, though.\nDatabase management matters. When things go wrong, knowing how to access the PostgreSQL database directly is valuable. It\u0026rsquo;s not something you do often, but when you need it, you really need it.\nWhy It Matters# This project is about taking back control of personal data. Every photo I take doesn\u0026rsquo;t need to be analyzed by Google\u0026rsquo;s AI or used to train their models. The photos are mine, stored on my hardware, accessible only to people I choose.\nIt\u0026rsquo;s also about long-term ownership. Cloud services change terms, raise prices, or shut down. With Immich, I own the infrastructure. If the project disappeared tomorrow, I\u0026rsquo;d still have all my photos and could migrate to something else.\nThe system has become essential for family events. Instead of texting compressed photos or using some sketchy file-sharing service, I create an Immich album and share a link. Everyone gets the full-resolution originals, and I can see who\u0026rsquo;s accessed the album.\n"},{"id":6,"href":"/docs/projects/miniflux/","title":"Miniflux","section":"Projects","content":"Miniflux RSS# I\u0026rsquo;m really not a fan of news and social feeds. I\u0026rsquo;ve gone on and off of RSS since 2010ish, trying different readers and whatnot, but always ending up at a different feed.\nMiniflux is nice, it does everything any RSS reader does, but gives a lot more flexibility because it\u0026rsquo;s its own backend. The global feed is manageable and the categories are clean.\nFlexibility comes in writing feed-specific rules.\nrewrite(\u0026#34;^https:\\/\\/(x\\.com|twitter\\.com)\\/(.*)$\u0026#34;|\u0026#34;https://xcancel.com/$2\u0026#34;),rewrite(\u0026#34;^https:\\/\\/(www\\.)?reddit\\.com\\/(.*)$\u0026#34;|\u0026#34;https://redlib.catsarch.com/$2\u0026#34;)These rules let you use nitter, xcancel, or whatever else to get around things. Reddit doesn\u0026rsquo;t like VPNs and as much as I try to abstain from Reddit, the millions of users can\u0026rsquo;t help but post answers to my problems only on this one reddit thread.\nRSS Feeds I follow:# Amazon Company News - https://www.aboutamazon.com/rss/feed.rss?category=company-news Amazon Retail - https://www.aboutamazon.com/rss/feed.rss?category=retail Android - https://blog.google/products-and-platforms/platforms/android/rss/ Marginal Revolution - https://feeds.feedblitz.com/marginalrevolution Hackaday - https://hackaday.com/blog/feed/ Hacker News Front Page - https://hnrss.org/frontpage Hacker News Newest - https://hnrss.org/newest\n"},{"id":7,"href":"/docs/projects/ollama/","title":"Ollama","section":"Projects","content":"Local AI with Ollama and Open WebUI# Running AI models locally instead of sending every query to OpenAI or Anthropic was the goal. Ollama makes it possible to run large language models on your own hardware, and Open WebUI provides a ChatGPT-like interface for interacting with them.\nThis setup means I can experiment with AI without worrying about API costs, rate limits, or sending sensitive data to third parties. The models run entirely on my server, using the GPU for acceleration.\nHow It Works# Ollama is the engine that runs the models. It handles downloading models, loading them into memory, and serving them through an API. Models are stored locally and can be swapped out easily—if I want to try a different model, I just pull it down and switch.\nOpen WebUI sits on top of Ollama and provides the chat interface. It looks and feels like ChatGPT but connects to my local Ollama instance instead of a cloud service. It supports multiple conversations, model switching, and even has features like document upload and web search integration.\nBoth run as Docker containers on my home server. Ollama gets access to the GPU for hardware acceleration, which is essential for running larger models at reasonable speeds. Without GPU acceleration, inference would be painfully slow.\nWorking with Kiro# Setting up Ollama was straightforward—it\u0026rsquo;s designed to be simple. The challenge was getting GPU acceleration working properly. AMD GPUs use ROCm instead of CUDA, which means different drivers and configuration. Kiro helped me figure out the right Docker setup to pass the GPU through to the container and verify that Ollama was actually using it.\nWe tested with different models to see what the hardware could handle. Smaller models like Llama 3.2 run fast, while larger models are slower but more capable. Finding the right balance between model size and performance took some experimentation.\nOpen WebUI was easier—it\u0026rsquo;s just a web interface that connects to Ollama\u0026rsquo;s API. The configuration is minimal, mostly just pointing it at the Ollama endpoint and setting up user accounts. We added it to the same Docker network as Ollama so they could communicate directly.\nThe interesting part was integrating it with other services. Open WebUI can connect to external APIs, which means it could potentially control Home Assistant or query other services on my network. That\u0026rsquo;s still experimental, but the foundation is there.\nWhat I Learned# Local AI is viable for many use cases. In some cases, my 14b or 27b model caught errors my API models made. Not the case often though, but I suspect the routing of these companies route smaller tasks to smaller models, understandably, but my 27b model performs 27b output every time.\nThe learning from LLMs in general\u0026hellip; is far beyond my normal weight class. It\u0026rsquo;s led to a lot of understanding of my professional workflow and tools, though. Understanding how they work, I have a deeper grasp of it than a lot of my non-tech peers. So, the endeavor is professional as much as it is enthustiastic.\nWhy It Matters# I have many thoughts about local LLM inferencing. Largest thoughts leading me to building a datacenter in my home and running 1T models locally. Smallest thoughts being\u0026hellip; well, pushed aside and dismissed.\n"}]