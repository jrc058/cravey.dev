<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Local AI with Ollama and Open WebUI# Running AI models locally instead of sending every query to OpenAI or Anthropic was the goal. Ollama makes it possible to run large language models on your own hardware, and Open WebUI provides a ChatGPT-like interface for interacting with them.
This setup means I can experiment with AI without worrying about API costs, rate limits, or sending sensitive data to third parties. The models run entirely on my server, using the GPU for acceleration.
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://cravey.dev/docs/projects/ollama/"><meta property="og:site_name" content="Cravey.dev"><meta property="og:title" content="Cravey.dev"><meta property="og:description" content="Local AI with Ollama and Open WebUI# Running AI models locally instead of sending every query to OpenAI or Anthropic was the goal. Ollama makes it possible to run large language models on your own hardware, and Open WebUI provides a ChatGPT-like interface for interacting with them.
This setup means I can experiment with AI without worrying about API costs, rate limits, or sending sensitive data to third parties. The models run entirely on my server, using the GPU for acceleration."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Self-Hosting"><meta property="article:tag" content="Open-Source"><meta property="article:tag" content="Projects"><meta property="article:tag" content="Open-Webui"><meta property="article:tag" content="Ai"><meta itemprop=name content="Cravey.dev"><meta itemprop=description content="Local AI with Ollama and Open WebUI# Running AI models locally instead of sending every query to OpenAI or Anthropic was the goal. Ollama makes it possible to run large language models on your own hardware, and Open WebUI provides a ChatGPT-like interface for interacting with them.
This setup means I can experiment with AI without worrying about API costs, rate limits, or sending sensitive data to third parties. The models run entirely on my server, using the GPU for acceleration."><meta itemprop=wordCount content="539"><meta itemprop=keywords content="Ollama,Self-Hosting,Open-Source,Projects,Open-Webui,Ai,Llm"><title>Ollama | Cravey.dev</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://cravey.dev/docs/projects/ollama/><link rel=stylesheet href=/book.min.6970156cec683193d93c9c4edaf0d56574e4361df2e0c1be4f697ae81c3ba55f.css integrity="sha256-aXAVbOxoMZPZPJxO2vDVZXTkNh3y4MG+T2l66Bw7pV8=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.e906fe123ddd357e988492e54ccaf4aa536c6d398ba61e9bf2d64e528661db8a.js integrity="sha256-6Qb+Ej3dNX6YhJLlTMr0qlNsbTmLph6b8tZOUoZh24o=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><a class="flex align-center" href=/ style=flex-direction:column><img src=/images/profile.png alt=Logo><h1 class=book-brand-title>Cravey.dev</h1><p class=book-brand-subtitle>Customer Success | Amazon | Self-Host Enthusiast | Digital Sovereignty Advocate</p></a><style>.book-brand-subtitle{font-size:.8rem;color:var(--color-text-secondary);margin-top:.25rem;text-align:center}.book-brand-title{margin-bottom:0!important}</style><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/workflows/>Workflows</a><ul><li><a href=/docs/workflows/image-embeds/>Embed images into cravey.dev</a></li><li><a href=/docs/workflows/living-server-documentation/>Creating Living Server Documentation: An Automated Workflow</a></li></ul></li><li><a href=/docs/about/>About</a><ul></ul></li><li><a href=/docs/projects/>Projects</a><ul><li><a href=/docs/projects/cyberdecktech/>CyberDeck Tech</a></li><li><a href=/docs/projects/homeassistant/>Homeassistant</a></li><li><a href=/docs/projects/immich/>Immich</a></li><li><a href=/docs/projects/miniflux/>Miniflux</a></li><li><a href=/docs/projects/ollama/ class=active>Ollama</a></li></ul></li><li><a href=/docs/posts/>Posts</a><ul><li><a href=/docs/posts/fallacies/>Fallacies</a></li></ul></li><li><a href=/docs/recipes/>Recipes</a><ul><li><a href=/docs/recipes/amishoatmeal/>Amish Oatmeal</a></li><li><a href=/docs/recipes/applecrisp/>Apple Crisp</a></li><li><a href=/docs/recipes/bananabread/>Banana Bread</a></li><li><a href=/docs/recipes/beans/>Beans</a></li><li><a href=/docs/recipes/biscotti/>Biscotti</a></li><li><a href=/docs/recipes/biscuits/>Biscuits</a></li><li><a href=/docs/recipes/braisedbeefneckbonestew/>Braised Beef Neck Bone Stew</a></li><li><a href=/docs/recipes/chickencacciatore/>Chicken Cacciatore</a></li><li><a href=/docs/recipes/chocolatepudding/>Chocolate Pudding</a></li><li><a href=/docs/recipes/cinnamonrolls/>Cinnamon Rolls</a></li><li><a href=/docs/recipes/coconutricethaibasilbeef/>Coconut Rice Thai Basil Beef</a></li><li><a href=/docs/recipes/dinnerrolls/>Dinner Rolls</a></li><li><a href=/docs/recipes/eggpasta/>Egg Pasta</a></li><li><a href=/docs/recipes/keylimepie/>Key Lime Pie</a></li><li><a href=/docs/recipes/leftoverfruitjam/>Leftover Fruit Jam</a></li><li><a href=/docs/recipes/lemonbutterpasta/>Lemon Butter Pasta</a></li><li><a href=/docs/recipes/lemoncapellini/>Lemon Capellini</a></li><li><a href=/docs/recipes/lemongarlicroastedchicken/>Lemon Garlic Roasted Chicken</a></li><li><a href=/docs/recipes/madeuptomatosauce/>Made Up Tomato Sauce</a></li><li><a href=/docs/recipes/meatloaf/>Meatloaf</a></li><li><a href=/docs/recipes/mexicanrice/>Mexican Rice</a></li><li><a href=/docs/recipes/mozzarellacheese/>Mozzarella Cheese</a></li><li><a href=/docs/recipes/panangcurry/>Panang Curry</a></li><li><a href=/docs/recipes/pancakes/>Pancakes</a></li><li><a href=/docs/recipes/pattymelt/>Patty Melt</a></li><li><a href=/docs/recipes/pecanbiscotti/>Pecan Biscotti</a></li><li><a href=/docs/recipes/picanha/>Picanha</a></li><li><a href=/docs/recipes/reversesearsteak/>Reverse Sear Steak</a></li><li><a href=/docs/recipes/sandwichbread/>Sandwich Bread</a></li><li><a href=/docs/recipes/sardinepasta/>Sardine Pasta</a></li><li><a href=/docs/recipes/scones/>Scones</a></li><li><a href=/docs/recipes/snowballcookies/>Snowball Cookies</a></li><li><a href=/docs/recipes/stovetopsweetenedcondensedmilk/>Stovetop Sweetened Condensed Milk</a></li><li><a href=/docs/recipes/vanillacustard/>Vanilla Custard</a></li><li><a href=/docs/recipes/waffles/>Waffles</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=/icons/menu.svg class=book-icon alt=Menu></label><h3>Ollama</h3><label for=toc-control><img src=/icons/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class=hidden><nav id=TableOfContents><ul><li><a href=#how-it-works>How It Works</a></li><li><a href=#working-with-kiro>Working with Kiro</a></li><li><a href=#what-i-learned>What I Learned</a></li><li><a href=#why-it-matters>Why It Matters</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=local-ai-with-ollama-and-open-webui>Local AI with Ollama and Open WebUI<a class=anchor href=#local-ai-with-ollama-and-open-webui>#</a></h1><p>Running AI models locally instead of sending every query to OpenAI or Anthropic was the goal. <a href=https://ollama.ai/>Ollama</a> makes it possible to run large language models on your own hardware, and <a href=https://openwebui.com/>Open WebUI</a> provides a ChatGPT-like interface for interacting with them.</p><p>This setup means I can experiment with AI without worrying about API costs, rate limits, or sending sensitive data to third parties. The models run entirely on my server, using the GPU for acceleration.</p><h2 id=how-it-works>How It Works<a class=anchor href=#how-it-works>#</a></h2><p>Ollama is the engine that runs the models. It handles downloading models, loading them into memory, and serving them through an API. Models are stored locally and can be swapped out easily—if I want to try a different model, I just pull it down and switch.</p><p>Open WebUI sits on top of Ollama and provides the chat interface. It looks and feels like ChatGPT but connects to my local Ollama instance instead of a cloud service. It supports multiple conversations, model switching, and even has features like document upload and web search integration.</p><p>Both run as Docker containers on my home server. Ollama gets access to the GPU for hardware acceleration, which is essential for running larger models at reasonable speeds. Without GPU acceleration, inference would be painfully slow.</p><h2 id=working-with-kiro>Working with Kiro<a class=anchor href=#working-with-kiro>#</a></h2><p>Setting up Ollama was straightforward—it&rsquo;s designed to be simple. The challenge was getting GPU acceleration working properly. AMD GPUs use ROCm instead of CUDA, which means different drivers and configuration. Kiro helped me figure out the right Docker setup to pass the GPU through to the container and verify that Ollama was actually using it.</p><p>We tested with different models to see what the hardware could handle. Smaller models like Llama 3.2 run fast, while larger models are slower but more capable. Finding the right balance between model size and performance took some experimentation.</p><p>Open WebUI was easier—it&rsquo;s just a web interface that connects to Ollama&rsquo;s API. The configuration is minimal, mostly just pointing it at the Ollama endpoint and setting up user accounts. We added it to the same Docker network as Ollama so they could communicate directly.</p><p>The interesting part was integrating it with other services. Open WebUI can connect to external APIs, which means it could potentially control Home Assistant or query other services on my network. That&rsquo;s still experimental, but the foundation is there.</p><h2 id=what-i-learned>What I Learned<a class=anchor href=#what-i-learned>#</a></h2><p>Local AI is viable for many use cases. In some cases, my 14b or 27b model caught errors my API models made. Not the case often though, but I suspect the routing of these companies route smaller tasks to smaller models, understandably, but my 27b model performs 27b output every time.</p><p>The learning from LLMs in general&mldr; is far beyond my normal weight class. It&rsquo;s led to a lot of understanding of my professional workflow and tools, though. Understanding how they work, I have a deeper grasp of it than a lot of my non-tech peers. So, the endeavor is professional as much as it is enthustiastic.</p><h2 id=why-it-matters>Why It Matters<a class=anchor href=#why-it-matters>#</a></h2><p>I have many thoughts about local LLM inferencing. Largest thoughts leading me to building a datacenter in my home and running 1T models locally. Smallest thoughts being&mldr; well, pushed aside and dismissed.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=/docs/projects/miniflux/ class="flex align-center"><img src=/icons/backward.svg class=book-icon alt=Backward>
<span>Miniflux</span>
</a></span><span><a href=/docs/posts/ class="flex align-center"><span>Posts</span>
<img src=/icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#how-it-works>How It Works</a></li><li><a href=#working-with-kiro>Working with Kiro</a></li><li><a href=#what-i-learned>What I Learned</a></li><li><a href=#why-it-matters>Why It Matters</a></li></ul></nav></div></aside></main></body></html>